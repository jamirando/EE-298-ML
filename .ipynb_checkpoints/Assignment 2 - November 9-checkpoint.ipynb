{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Activations\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "## Backward Activations\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sigmoid_Z = sigmoid(Z)\n",
    "    return dA * sigmoid_Z * (1 - sigmoid_Z)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <=0] = 0\n",
    "    return dZ\n",
    "    \n",
    "## Initialize Layers\n",
    "def initialize_layers(network_architecture):\n",
    "    number_of_layers = len(network_architecture)\n",
    "    parameters = {}\n",
    "    \n",
    "    for index, layer in enumerate(network_architecture):\n",
    "        layer_index = index + 1\n",
    "        layer_input_size = layer[\"input_dimension\"]\n",
    "        layer_output_size = layer[\"output_dimension\"]\n",
    "        \n",
    "        ##### remove *0.1  to parameter initialization #####\n",
    "        parameters['W' + str(layer_index)] = np.random.randn(layer_output_size, layer_input_size) * 0.1\n",
    "        parameters['b' + str(layer_index)] = np.random.randn(layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "def single_layer_forward_propagation(A_previous, W_current, b_current, activation=\"relu\"):\n",
    "    Z_current = np.dot(W_current, A_previous) + b_current\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_function = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_function = sigmoid\n",
    "    else:\n",
    "        raiseException('Unsupported activation')\n",
    "\n",
    "    return activation_function(Z_current), Z_current\n",
    "\n",
    "def full_forward_propagation(X, parameters, network_architecture):\n",
    "    memory = {}\n",
    "    A_current = X\n",
    "    \n",
    "    for index, layer in enumerate(network_architecture):\n",
    "        layer_index = index + 1\n",
    "        A_previous = A_current\n",
    "        \n",
    "        activation_function_current = layer[\"activation\"]\n",
    "        W_current = parameters[\"W\" + str(layer_index)]\n",
    "        b_current = parameters[\"b\" + str(layer_index)]\n",
    "        A_current, Z_current = single_layer_forward_propagation(A_previous, W_current, b_current, activation_function_current)\n",
    "\n",
    "        memory[\"A\" + str(index)] = A_previous\n",
    "        memory[\"Z\" + str(layer_index)] = Z_current\n",
    "        \n",
    "    return A_current, memory\n",
    "\n",
    "## Loss\n",
    "##### Check for correct implementation #####\n",
    "def MSE(Y_hat, Y):\n",
    "    m =  Y_hat.shape[1]\n",
    "    loss = 1/m * np.sum(np.power(Y_hat - Y,2))\n",
    "    return np.squeeze(loss)\n",
    "#     loss = np.power(Y_hat - Y,2).mean()\n",
    "    \n",
    "#     return loss\n",
    "    \n",
    "# def BCE(Y_hat, Y):\n",
    "#     m = Y_hat.shape[1]\n",
    "#     loss = -1/m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1-Y_hat).T))\n",
    "    \n",
    "#     return np.squeeze(loss)\n",
    "\n",
    "## Convert Probabilities to Classes\n",
    "##### Should be one hot vectors ######\n",
    "def convert_probabilities_to_classes(probabilities):\n",
    "    classes = np.copy(probabilities)\n",
    "    classes[probabilities > 0.5] = 1\n",
    "    classes[probabilities <= 0.5] = 0\n",
    "    \n",
    "    return classes\n",
    "    \n",
    "## Accuracy\n",
    "def Accuracy(Y_hat, Y):\n",
    "    Y_hat_class = convert_probabilities_to_classes(Y_hat)\n",
    "    return (Y_hat_class == Y).all(axis=0).mean()\n",
    "    \n",
    "## Backward Propagation\n",
    "##### MIGHT BE CHANGED TO FOLLOW DISCUSSION'S IMPLEMENTATION #####\n",
    "def single_layer_backward_propagation(dA_current, W_current, b_current, Z_current, A_previous, activation=\"relu\"):\n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_activation_function = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_function = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Unsupported activation')\n",
    "        \n",
    "    dZ_current = backward_activation_function(dA_current, Z_current)\n",
    "    \n",
    "    dW_current = np.dot(dZ_current, np.transpose(A_previous)) / m\n",
    "    db_current = np.sum(dZ_current, axis=1, keepdims=True) / m\n",
    "    dA_previous = np.dot(np.transpose(W_current), dZ_current)\n",
    "    \n",
    "    return dA_previous, dW_current, db_current\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, parameters, network_architecture):\n",
    "    gradients = {}\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    dA_previous = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
    "    \n",
    "    for layer_index_previous, layer in reversed(list(enumerate(network_architecture))):\n",
    "        layer_index_current = layer_index_previous + 1\n",
    "        activation_function_current = layer[\"activation\"]\n",
    "        \n",
    "        dA_current = dA_previous\n",
    "        \n",
    "        A_previous = memory[\"A\" + str(layer_index_previous)]\n",
    "        Z_current = memory[\"Z\" + str(layer_index_current)]\n",
    "        \n",
    "        W_current = parameters[\"W\" + str(layer_index_current)]\n",
    "        b_current = parameters[\"b\" + str(layer_index_current)]\n",
    "        \n",
    "        dA_previous, dW_current, db_current = single_layer_backward_propagation(dA_current, W_current, b_current, Z_current, A_previous, activation_function_current)\n",
    "        \n",
    "        gradients[\"dW\" + str(layer_index_current)] = dW_current\n",
    "        gradients[\"db\" + str(layer_index_current)] = db_current\n",
    "        \n",
    "    return gradients\n",
    "\n",
    "def update(parameters, gradients, network_architecture, learning_rate):\n",
    "    \n",
    "    for layer_index, layer in enumerate(network_architecture, 1):\n",
    "        parameters[\"W\" + str(layer_index)] -= learning_rate * gradients[\"dW\" + str(layer_index)]\n",
    "        parameters[\"b\" + str(layer_index)] -= learning_rate * gradients[\"db\" + str(layer_index)]\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def train(X, Y, network_architecture, epochs, learning_rate):\n",
    "    parameters = initialize_layers(network_architecture)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cache = full_forward_propagation(X, parameters, network_architecture)\n",
    "        \n",
    "        ##### loss should be MSE ######\n",
    "        loss = MSE(Y_hat, Y)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        accuracy = Accuracy(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        gradients = full_backward_propagation(Y_hat, Y, cache, parameters, network_architecture)\n",
    "        \n",
    "        parameters = update(parameters, gradients, network_architecture, learning_rate)\n",
    "        \n",
    "        print(\"Iteration: {:05} - loss: {:.5f} - accuracy: {:.5f}\".format(i, loss, accuracy))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Architecture\n",
    "network_architecture = [\n",
    "    {\"input_dimension\":1, \"output_dimension\":64, \"activation\":\"relu\"},\n",
    "    {\"input_dimension\":64, \"output_dimension\":64, \"activation\":\"relu\"},\n",
    "    {\"input_dimension\":64, \"output_dimension\":1, \"activation\":\"sigmoid\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Mean:5\n",
      "Enter Standard Deviation:5\n",
      "(900,) (100,)\n"
     ]
    }
   ],
   "source": [
    "mean_text = input(\"Enter Mean:\")\n",
    "mean = float(mean_text)\n",
    "\n",
    "standard_deviation_text = input(\"Enter Standard Deviation:\")\n",
    "standard_deviation = float(standard_deviation_text)\n",
    "\n",
    "number_of_samples = 1000\n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "##### Fix Dataset Generation - should be between [-2*std,2*std] #####\n",
    "D = np.random.normal(mean, standard_deviation, (number_of_samples,1))\n",
    "\n",
    "D_train = D[0:int(0.9*number_of_samples)]\n",
    "D_test = D[int(-0.1*number_of_samples):number_of_samples]\n",
    "\n",
    "print(D_train.shape, D_test.shape)\n",
    "\n",
    "# from sklearn.datasets import make_moons\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = make_moons(n_samples = number_of_samples, noise=0.2, random_state=100)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=42)\n",
    "\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (64,1) and (900,) not aligned: 1 (dim 1) != 900 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f0b4a6690196>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_architecture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-b86f73c0ccad>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, network_architecture, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mY_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_architecture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m##### loss should be MSE ######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b86f73c0ccad>\u001b[0m in \u001b[0;36mfull_forward_propagation\u001b[1;34m(X, parameters, network_architecture)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mW_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mb_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mA_current\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_previous\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_current\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_current\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_function_current\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"A\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA_previous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-b86f73c0ccad>\u001b[0m in \u001b[0;36msingle_layer_forward_propagation\u001b[1;34m(A_previous, W_current, b_current, activation)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msingle_layer_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_previous\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_current\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_current\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mZ_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_current\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_previous\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_current\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (64,1) and (900,) not aligned: 1 (dim 1) != 900 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "parameters = train(np.transpose(D_train), np.transpose(D_train), network_architecture, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), parameters, network_architecture)\n",
    "\n",
    "# Accuracy achieved on the test set\n",
    "acc_test = Accuracy(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0],1))))\n",
    "print(\"Test Set Accuracy: {:.2f}\".format(acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
