{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for creating the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Activations and Layer Initialization\n",
    "\n",
    "Activations\n",
    " - Sigmoid\n",
    " - RELU\n",
    " \n",
    "Layers are initialized by random samples from a standard normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Activations\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "## Backward Activations\n",
    "def sigmoid_backward(dA, Z):\n",
    "    return dA * sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <=0] = 0\n",
    "    return dZ\n",
    "    \n",
    "## Initialize Layers\n",
    "def initialize_layers(network_architecture):\n",
    "    '''\n",
    "    Initializes the weights and biases of all layers to random samples from the standard normal distribution\n",
    "\n",
    "    Input: \n",
    "        network_architecture: List of layers following the format - {input_dimesnsion: XX output_dimension: XX activation:\"XX\"}\n",
    "    Output: \n",
    "        parameters: Dictionary with keys Wx and bx containing the parameter values for the weights and biases of all layers\n",
    "    '''\n",
    "    number_of_layers = len(network_architecture)\n",
    "    parameters = {}\n",
    "    \n",
    "    for index, layer in enumerate(network_architecture):\n",
    "        layer_index = index + 1\n",
    "        layer_input_size = layer[\"input_dimension\"]\n",
    "        layer_output_size = layer[\"output_dimension\"]\n",
    "        \n",
    "        parameters['W' + str(layer_index)] = np.random.randn(layer_output_size, layer_input_size) *0.1\n",
    "        parameters['b' + str(layer_index)] = np.random.randn(layer_output_size, 1) *0.1\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "The Forward pass step is split into two functions: The single_layer_forward_propagation() and full_forward_propagation(). \n",
    "\n",
    "single_layer_forward_propagation() computes for the output of the layer with or without activations.\n",
    "This function solves the equations : \n",
    "\\begin{equation}\n",
    "Z_n = W_n \\cdot A_{n-1} + b_n\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "A_n = \\sigma(Z_n)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "full_forward_propagation() implements the full forward pass using the single_layer_forward_propagation() function and aggregates all forward pass outputs of each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Propagation\n",
    "def single_layer_forward_propagation(A_previous, W_current, b_current, activation=\"relu\"):\n",
    "    '''\n",
    "    Implements the forward pass of a single layer\n",
    "    \n",
    "    Inputs:\n",
    "        A_previous: Output of the previous (For the first layer, this is the input to the network)\n",
    "        W_current: Weights of the current layer\n",
    "        b_current: Biases of the current layer\n",
    "        activation: activation of the current layer\n",
    "    Outputs:\n",
    "        Z_current: Output of the current layer\n",
    "        activation_function(Z_current): Output of the current layer with activation\n",
    "    '''\n",
    "    \n",
    "    Z_current = np.dot(W_current, A_previous) + b_current\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_function = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_function = sigmoid\n",
    "    else:\n",
    "        raiseException('Unsupported activation')\n",
    "\n",
    "    return activation_function(Z_current), Z_current\n",
    "\n",
    "def full_forward_propagation(X, parameters, network_architecture):\n",
    "    '''\n",
    "    Implements the full forward pass usin the single_layer_forward_propagation() function\n",
    "    \n",
    "    Inputs:\n",
    "        X: Input to the network\n",
    "        parameters: Dictionary with keys Wx and bx containing the parameter values for the weights and biases of all layers\n",
    "        network_architecture: List of layers following the format - {input_dimesnsion: XX output_dimension: XX activation:\"XX\"}\n",
    "    Output:\n",
    "        A_current: The final output of last layer (predictions) \n",
    "        memory: Stores all the forward pass outputs of each layer.\n",
    "    '''\n",
    "    \n",
    "    memory = {}\n",
    "    A_current = X\n",
    "    \n",
    "    for index, layer in enumerate(network_architecture):\n",
    "        layer_index = index + 1\n",
    "        A_previous = A_current\n",
    "        \n",
    "        activation_function_current = layer[\"activation\"]\n",
    "        W_current = parameters[\"W\" + str(layer_index)]\n",
    "        b_current = parameters[\"b\" + str(layer_index)]\n",
    "        A_current, Z_current = single_layer_forward_propagation(A_previous, W_current, b_current, activation_function_current)\n",
    "\n",
    "        memory[\"A\" + str(index)] = A_previous\n",
    "        memory[\"Z\" + str(layer_index)] = Z_current\n",
    "        \n",
    "    return A_current, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Performance Metrics\n",
    "\n",
    "Loss Function:\n",
    " - Mean Square Error (MSE)\n",
    " \n",
    "Performance Metrics:\n",
    " - L1 distance\n",
    " - L2 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss\n",
    "def MSE(Y_hat, Y):\n",
    "    '''\n",
    "    Computes the Mean Square Error \n",
    "    \n",
    "    Inputs:\n",
    "        Y_hat: The predicted value\n",
    "        Y: The ground truth value\n",
    "    Output:\n",
    "        loss: Mean Square Error\n",
    "    \n",
    "    '''\n",
    "    loss = np.square(Y_hat - Y).mean()\n",
    "    return loss\n",
    "\n",
    "## Performance Metric\n",
    "def l2_distance(Y_hat, Y):\n",
    "    '''\n",
    "    Computes the L2 distance\n",
    "    \n",
    "    Inputs:\n",
    "        Y_hat: The predicted value\n",
    "        Y: The ground truth value\n",
    "    Output:\n",
    "        distance: l2_distance\n",
    "    '''\n",
    "    \n",
    "    distance = np.sqrt(np.square(Y_hat - Y).mean())\n",
    "    return distance\n",
    "\n",
    "def l1_distance(Y_hat, Y):\n",
    "    '''\n",
    "    Computes the L1 distance\n",
    "    \n",
    "    Inputs:\n",
    "        Y_hat: The predicted value\n",
    "        Y: The ground truth value\n",
    "    Output:\n",
    "        distance: l1_distance\n",
    "    '''\n",
    "    \n",
    "    distance = np.abs(Y_hat - Y).mean()\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "\n",
    "The backward propagation step is split into two functions: The single_layer_backward_propagation() and the full_backward_propagation().\n",
    "\n",
    "single_layer_backward_propagation() computes not only the gradients for the current layer's weights and biases but also the input to be used for the previous layer.\n",
    "This function solves the equations:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial Z_n} =  \\frac{\\partial L}{\\partial A_n} * \\frac{\\partial A_n}{\\partial Z_n}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W_n} = \\frac{\\partial L}{\\partial Z_n} \\cdot A_{n-1} \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial b_n} = \\frac{\\partial L}{\\partial Z_n}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial A_{n-1}} = \\frac{\\partial L}{\\partial Z_n} \\cdot W_{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "full_backward_propagation() initializes the backward propagation and does the backward propagation for the whole network also aggregates the gradients for all layers.\n",
    "\n",
    "update() performs the gradient descent on the weights and biases using the gradients from full backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward Propagation\n",
    "def single_layer_backward_propagation(dA_current, W_current, b_current, Z_current, A_previous, activation=\"relu\"):\n",
    "    '''\n",
    "    Implements a single layer backward propagation\n",
    "    \n",
    "    Note:\n",
    "    Assume: backprop for last layer (layer 3),\n",
    "    dA_current for the last layer is dL/dy_hat\n",
    "    In our case where L is MSE, dL/dy_hat is 2(y - y_hat)\n",
    "    the output for this layer would be the input to the layer before this (layer 2)\n",
    "    \n",
    "    \n",
    "    Inputs: \n",
    "        dA_current: backward pass input for the current layer \n",
    "        W_current: Weights of the current layer\n",
    "        b_current: Biases of the current layer\n",
    "        Z_current: forward pass output of the current layer\n",
    "        A_previous: forward pass output with activation of the previous layer\n",
    "    \n",
    "    Outputs:\n",
    "        dA_previous: backward pass output to be used as input for the previous layer (dL/dA)\n",
    "        dW_current: gradients for weights (dL/dW)\n",
    "        db_current: gradients for biases (dL/db)\n",
    "    '''\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_activation_function = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_function = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Unsupported activation')\n",
    "        \n",
    "    dZ_current = backward_activation_function(dA_current, Z_current)\n",
    "    \n",
    "    dW_current = np.dot(dZ_current, np.transpose(A_previous))\n",
    "    db_current = np.sum(dZ_current, axis=1, keepdims=True) \n",
    "    dA_previous = np.dot(np.transpose(W_current), dZ_current)\n",
    "    \n",
    "    return dA_previous, dW_current, db_current\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, parameters, network_architecture):\n",
    "    '''\n",
    "    Implements the full backward propagation.\n",
    "    \n",
    "    Inputs:\n",
    "        Y_hat: predictions of the forward pass\n",
    "        Y: ground truth\n",
    "        memory: Stores all forward pass outputs of each layer \n",
    "        parameters: Dictionary with keys Wx and bx containing the parameter values for the weights and biases of all layers\n",
    "        network_architecture: List of layers following the format - {input_dimesnsion: XX output_dimension: XX activation:\"XX\"}\n",
    "    Output:\n",
    "        gradient: gradients for all layers        \n",
    "    '''\n",
    "    \n",
    "    gradients = {}\n",
    "    \n",
    "    dA_previous = 2 * (Y_hat-Y)\n",
    "    \n",
    "    for layer_index_previous, layer in reversed(list(enumerate(network_architecture))):\n",
    "        layer_index_current = layer_index_previous + 1\n",
    "        activation_function_current = layer[\"activation\"]\n",
    "        \n",
    "        dA_current = dA_previous\n",
    "        \n",
    "        A_previous = memory[\"A\" + str(layer_index_previous)]\n",
    "        Z_current = memory[\"Z\" + str(layer_index_current)]\n",
    "        \n",
    "        W_current = parameters[\"W\" + str(layer_index_current)]\n",
    "        b_current = parameters[\"b\" + str(layer_index_current)]\n",
    "        \n",
    "        dA_previous, dW_current, db_current = single_layer_backward_propagation(dA_current, W_current, b_current, Z_current, A_previous, activation_function_current)\n",
    "        \n",
    "        gradients[\"dW\" + str(layer_index_current)] = dW_current\n",
    "        gradients[\"db\" + str(layer_index_current)] = db_current\n",
    "        \n",
    "    return gradients\n",
    "\n",
    "def update(parameters, gradients, network_architecture, learning_rate):\n",
    "    '''\n",
    "    Updates the parameters based on the gradients and learning rate\n",
    "    \n",
    "    Inputs:\n",
    "        parameters: Dictionary with keys Wx and bx containing the parameter values for the weights and biases of all layers \n",
    "        gradients: gradients for all layers\n",
    "        network_architecture: List of layers following the format - {input_dimesnsion: XX output_dimension: XX activation:\"XX\"}\n",
    "        learning_rate: learning rate for update\n",
    "    Output\n",
    "        parameters: Updated dictionary of parameter values for the weights and biases of all layers\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    for layer_index, layer in enumerate(network_architecture,1):\n",
    "        parameters[\"W\" + str(layer_index)] -= learning_rate * gradients[\"dW\" + str(layer_index)]\n",
    "        parameters[\"b\" + str(layer_index)] -= learning_rate * gradients[\"db\" + str(layer_index)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "Describes how the network looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Architecture\n",
    "network_architecture = [\n",
    "    {\"input_dimension\":1, \"output_dimension\":64, \"activation\":\"relu\"},\n",
    "    {\"input_dimension\":64, \"output_dimension\":64, \"activation\":\"relu\"},\n",
    "    {\"input_dimension\":64, \"output_dimension\":1, \"activation\":\"sigmoid\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, network_architecture, epochs, learning_rate):\n",
    "    '''\n",
    "    Training code\n",
    "    \n",
    "    Inputs: \n",
    "        X: training data\n",
    "        Y: ground truth\n",
    "        network_architecture: List of layers following the format - {input_dimesnsion: XX output_dimension: XX activation:\"XX\"}\n",
    "        epoch: number of training epochs\n",
    "        learning_rate: learning rate for gradient update\n",
    "    '''\n",
    "    \n",
    "    parameters = initialize_layers(network_architecture)\n",
    "    \n",
    "    l1_distance_history = []\n",
    "    l2_distance_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_history = []\n",
    "        Y_hat_epoch = []\n",
    "        for i in range(len(X)):\n",
    "            Y_hat, cache = full_forward_propagation(X[i].reshape(1,1), parameters, network_architecture)\n",
    "            Y_hat_epoch.append(Y_hat)\n",
    "\n",
    "            loss = MSE(Y_hat, Y[i])\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            gradients = full_backward_propagation(Y_hat, Y[i], cache, parameters, network_architecture)\n",
    "\n",
    "            parameters = update(parameters, gradients, network_architecture, learning_rate)\n",
    "\n",
    "        distance_l1 = l1_distance(np.array(Y_hat_epoch).reshape(900,), Y)\n",
    "        l1_distance_history.append(distance_l1)\n",
    "\n",
    "        distance_l2 = l2_distance(np.array(Y_hat_epoch).reshape(900,), Y)\n",
    "        l2_distance_history.append(distance_l2)\n",
    "        \n",
    "        print(\"Epoch: {:02} - loss: {:.5f} - l1-distance: {:.5f} - l2-distance: {:.5f}\".format(epoch+1, loss.mean(), distance_l1 , distance_l2))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset generated by requesting the user to provide the mean and standard deviation of a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Mean:0\n",
      "Enter Standard Deviation:1\n"
     ]
    }
   ],
   "source": [
    "mean = float(input(\"Enter Mean:\"))\n",
    "standard_deviation = float(input(\"Enter Standard Deviation:\"))\n",
    "\n",
    "number_of_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this normal distribution, 1,000,000 samples 2 standard deviations away from the mean are taken. \n",
    "\n",
    "We then create a histogram with 1000 bins and obtain the probabilities for each sample. We then reduce the dataset to 1000 iid samples.\n",
    "\n",
    "Minmax normalization is also applied to the data to ensure that all values would be between 0 to 1.\n",
    "\n",
    "The final step is to split the data into train and test sets with the following distribution. \n",
    "- Training Samples: 900\n",
    "- Test Samples: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Generation\n",
    "samples = []\n",
    "for i in range(1000000):\n",
    "    sample = np.random.normal(mean, standard_deviation)\n",
    "    while sample < mean - 2*standard_deviation or sample > mean + 2*standard_deviation:\n",
    "        sample = np.random.normal(mean, standard_deviation)\n",
    "        \n",
    "    samples.append(sample)\n",
    "samples = np.array(samples)\n",
    "\n",
    "probabilities, bins = np.histogram(samples, bins=number_of_samples, density=True)\n",
    "indices = np.digitize(samples, bins, right=True) - 1\n",
    "\n",
    "X_raw = samples[:number_of_samples]\n",
    "Y = probabilities[indices][:number_of_samples]\n",
    "X = (X_raw - np.min(X_raw))/(np.max(X_raw) - np.min(X_raw))\n",
    "\n",
    "X_train = X[:int(.9*number_of_samples)]\n",
    "Y_train = Y[:int(.9*number_of_samples)]\n",
    "X_test = X[int(.9*number_of_samples):]\n",
    "Y_test = Y[int(.9*number_of_samples):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- Batch size = 1\n",
    "- Epochs = 20 \n",
    "- Learning Rate = 0.1\n",
    "\n",
    "Per epoch loss, l1 distance and l2 distance are printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 - loss: 0.00068 - l1-distance: 0.08886 - l2-distance: 0.10719\n",
      "Epoch: 02 - loss: 0.00069 - l1-distance: 0.08632 - l2-distance: 0.10237\n",
      "Epoch: 03 - loss: 0.00066 - l1-distance: 0.08532 - l2-distance: 0.10120\n",
      "Epoch: 04 - loss: 0.00063 - l1-distance: 0.08383 - l2-distance: 0.09951\n",
      "Epoch: 05 - loss: 0.00059 - l1-distance: 0.08139 - l2-distance: 0.09676\n",
      "Epoch: 06 - loss: 0.00058 - l1-distance: 0.07712 - l2-distance: 0.09198\n",
      "Epoch: 07 - loss: 0.00055 - l1-distance: 0.06929 - l2-distance: 0.08328\n",
      "Epoch: 08 - loss: 0.00042 - l1-distance: 0.05622 - l2-distance: 0.06873\n",
      "Epoch: 09 - loss: 0.00013 - l1-distance: 0.03926 - l2-distance: 0.04980\n",
      "Epoch: 10 - loss: 0.00000 - l1-distance: 0.02559 - l2-distance: 0.03365\n",
      "Epoch: 11 - loss: 0.00004 - l1-distance: 0.01840 - l2-distance: 0.02439\n",
      "Epoch: 12 - loss: 0.00007 - l1-distance: 0.01459 - l2-distance: 0.01930\n",
      "Epoch: 13 - loss: 0.00007 - l1-distance: 0.01238 - l2-distance: 0.01631\n",
      "Epoch: 14 - loss: 0.00006 - l1-distance: 0.01110 - l2-distance: 0.01451\n",
      "Epoch: 15 - loss: 0.00003 - l1-distance: 0.01035 - l2-distance: 0.01338\n",
      "Epoch: 16 - loss: 0.00002 - l1-distance: 0.00986 - l2-distance: 0.01261\n",
      "Epoch: 17 - loss: 0.00001 - l1-distance: 0.00947 - l2-distance: 0.01207\n",
      "Epoch: 18 - loss: 0.00000 - l1-distance: 0.00919 - l2-distance: 0.01166\n",
      "Epoch: 19 - loss: 0.00000 - l1-distance: 0.00895 - l2-distance: 0.01133\n",
      "Epoch: 20 - loss: 0.00000 - l1-distance: 0.00874 - l2-distance: 0.01106\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "parameters = train(np.transpose(X_train), np.transpose(Y_train), network_architecture, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Benchmark\n",
    "\n",
    "Testing is done by doing a full forward propagation on the test set to get the predictions. We reuse the parameters we get from training when doing the forward propagation.\n",
    "\n",
    "L1 distance and L2 distance are used for benchmarking because they are the common metrics used for evaluation of regression models. MSE is also one of the common benchmarks but it was used as the loss function therefore using it as a metric would yield us no benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: \n",
      " l1-distance: 0.00819 \n",
      " l2-distance: 0.01017 \n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(X_test.reshape(X_test.shape[0],1)), parameters, network_architecture)\n",
    "\n",
    "# Distance achieved on the test set\n",
    "l1_distance_test = l1_distance(Y_test_hat, np.transpose(Y_test))\n",
    "l2_distance_test = l2_distance(Y_test_hat, np.transpose(Y_test))\n",
    "print(\"Test Set: \\n l1-distance: {:.5f} \\n l2-distance: {:.5f} \".format(l1_distance_test, l2_distance_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "matplotlib was used to visualize the predictions of the network versus the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(X_raw[int(.9*number_of_samples):],Y_test,'r.')\n",
    "plt.plot(X_raw[int(.9*number_of_samples):],np.transpose(Y_test_hat),'b.')\n",
    "plt.legend([\"ground truth\", \"predictions\"])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Probabilities')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1.  https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/03_numpy_neural_net/Numpy%20deep%20neural%20network.ipynb\n",
    "\n",
    "2. https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
