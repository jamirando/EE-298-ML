{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Activations\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "## Backward Activations\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sigmoid_Z = sigmoid(Z)\n",
    "    return dA * sigmoid_Z * (1 - sigmoid_Z)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <=0] = 0\n",
    "    return dZ\n",
    "    \n",
    "## Initialize Layers\n",
    "def initialize_layers(network_architecture):\n",
    "    number_of_layers = len(network_architecture)\n",
    "    parameters = {}\n",
    "    \n",
    "    for index, layer in enumerate(network_architecture):\n",
    "        layer_index = index + 1\n",
    "        layer_input_size = layer[\"input_dimension\"]\n",
    "        layer_output_size = layer[\"output_dimension\"]\n",
    "        \n",
    "        ##### remove *0.1  to parameter initialization #####\n",
    "        parameters['W' + str(layer_index)] = np.random.randn(layer_output_size, layer_input_size) * 0.1\n",
    "        parameters['b' + str(layer_index)] = np.random.randn(layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "def single_layer_forward_propagation(A_previous, W_current, b_current, activation=\"relu\"):\n",
    "    Z_current = np.dot(W_current, A_previous) + b_current\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_function = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_function = sigmoid\n",
    "    else:\n",
    "        raiseException('Unsupported activation')\n",
    "\n",
    "    return activation_function(Z_current), Z_current\n",
    "\n",
    "def full_forward_propagation(X, parameters, network_architecture):\n",
    "    memory = {}\n",
    "    A_current = X\n",
    "    \n",
    "    for index, layer in enumerate(network_architecture):\n",
    "        layer_index = index + 1\n",
    "        A_previous = A_current\n",
    "        \n",
    "        activation_function_current = layer[\"activation\"]\n",
    "        W_current = parameters[\"W\" + str(layer_index)]\n",
    "        b_current = parameters[\"b\" + str(layer_index)]\n",
    "        A_current, Z_current = single_layer_forward_propagation(A_previous, W_current, b_current, activation_function_current)\n",
    "\n",
    "        memory[\"A\" + str(index)] = A_previous\n",
    "        memory[\"Z\" + str(layer_index)] = Z_current\n",
    "        \n",
    "    return A_current, memory\n",
    "\n",
    "## Loss\n",
    "##### Check for correct implementation #####\n",
    "def MSE(Y_hat, Y):\n",
    "#     m =  Y_hat.shape[1]\n",
    "#     loss = 1/m * np.sum(np.power(Y_hat - Y,2))\n",
    "#     return np.squeeze(loss)\n",
    "    loss = np.power(Y_hat - Y,2).mean()\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "# def BCE(Y_hat, Y):\n",
    "#     m = Y_hat.shape[1]\n",
    "#     loss = -1/m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1-Y_hat).T))\n",
    "    \n",
    "#     return np.squeeze(loss)\n",
    "\n",
    "## Convert Probabilities to Classes\n",
    "##### Should be one hot vectors ######\n",
    "def convert_probabilities_to_classes(probabilities):\n",
    "    classes = np.copy(probabilities)\n",
    "    classes[probabilities > 0.5] = 1\n",
    "    classes[probabilities <= 0.5] = 0\n",
    "    \n",
    "    return classes\n",
    "    \n",
    "## Accuracy\n",
    "def Accuracy(Y_hat, Y):\n",
    "    Y_hat_class = convert_probabilities_to_classes(Y_hat)\n",
    "    return (Y_hat_class == Y).all(axis=0).mean()\n",
    "    \n",
    "## Backward Propagation\n",
    "##### MIGHT BE CHANGED TO FOLLOW DISCUSSION'S IMPLEMENTATION #####\n",
    "def single_layer_backward_propagation(dA_current, W_current, b_current, Z_current, A_previous, activation=\"relu\"):\n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_activation_function = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_function = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Unsupported activation')\n",
    "        \n",
    "    dZ_current = backward_activation_function(dA_current, Z_current)\n",
    "    \n",
    "    dW_current = np.dot(dZ_current, np.transpose(A_previous)) / m\n",
    "    db_current = np.sum(dZ_current, axis=1, keepdims=True) / m\n",
    "    dA_previous = np.dot(np.transpose(W_current), dZ_current)\n",
    "    \n",
    "    return dA_previous, dW_current, db_current\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, parameters, network_architecture):\n",
    "    gradients = {}\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    dA_previous = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
    "    \n",
    "    for layer_index_previous, layer in reversed(list(enumerate(network_architecture))):\n",
    "        layer_index_current = layer_index_previous + 1\n",
    "        activation_function_current = layer[\"activation\"]\n",
    "        \n",
    "        dA_current = dA_previous\n",
    "        \n",
    "        A_previous = memory[\"A\" + str(layer_index_previous)]\n",
    "        Z_current = memory[\"Z\" + str(layer_index_current)]\n",
    "        \n",
    "        W_current = parameters[\"W\" + str(layer_index_current)]\n",
    "        b_current = parameters[\"b\" + str(layer_index_current)]\n",
    "        \n",
    "        dA_previous, dW_current, db_current = single_layer_backward_propagation(dA_current, W_current, b_current, Z_current, A_previous, activation_function_current)\n",
    "        \n",
    "        gradients[\"dW\" + str(layer_index_current)] = dW_current\n",
    "        gradients[\"db\" + str(layer_index_current)] = db_current\n",
    "        \n",
    "    return gradients\n",
    "\n",
    "def update(parameters, gradients, network_architecture, learning_rate):\n",
    "    \n",
    "    for layer_index, layer in enumerate(network_architecture, 1):\n",
    "        parameters[\"W\" + str(layer_index)] -= learning_rate * gradients[\"dW\" + str(layer_index)]\n",
    "        parameters[\"b\" + str(layer_index)] -= learning_rate * gradients[\"db\" + str(layer_index)]\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def train(X, Y, network_architecture, epochs, learning_rate):\n",
    "    parameters = initialize_layers(network_architecture)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cache = full_forward_propagation(X, parameters, network_architecture)\n",
    "        \n",
    "        ##### loss should be MSE ######\n",
    "        loss = MSE(Y_hat, Y)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        accuracy = Accuracy(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        gradients = full_backward_propagation(Y_hat, Y, cache, parameters, network_architecture)\n",
    "        \n",
    "        parameters = update(parameters, gradients, network_architecture, learning_rate)\n",
    "        \n",
    "        print(\"Iteration: {:05} - loss: {:.5f} - accuracy: {:.5f}\".format(i, loss, accuracy))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Architecture\n",
    "network_architecture = [\n",
    "    {\"input_dimension\":1, \"output_dimension\":64, \"activation\":\"relu\"},\n",
    "    {\"input_dimension\":64, \"output_dimension\":64, \"activation\":\"relu\"},\n",
    "    {\"input_dimension\":64, \"output_dimension\":1, \"activation\":\"sigmoid\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Mean:2\n",
      "Enter Standard Deviation:1\n",
      "(900, 1) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "mean_text = input(\"Enter Mean:\")\n",
    "mean = float(mean_text)\n",
    "\n",
    "standard_deviation_text = input(\"Enter Standard Deviation:\")\n",
    "standard_deviation = float(standard_deviation_text)\n",
    "\n",
    "number_of_samples = 1000\n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "##### Fix Dataset Generation - should be between [-2*std,2*std] #####\n",
    "D = np.random.normal(mean, standard_deviation, (number_of_samples,1))\n",
    "\n",
    "D_train = D[0:int(0.9*number_of_samples)]\n",
    "D_test = D[int(-0.1*number_of_samples):number_of_samples]\n",
    "\n",
    "print(D_train.shape, D_test.shape)\n",
    "\n",
    "# from sklearn.datasets import make_moons\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = make_moons(n_samples = number_of_samples, noise=0.2, random_state=100)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=42)\n",
    "\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - loss: 3.29859 - accuracy: 0.00000\n",
      "Iteration: 00001 - loss: 2.98993 - accuracy: 0.00000\n",
      "Iteration: 00002 - loss: 2.74291 - accuracy: 0.00000\n",
      "Iteration: 00003 - loss: 2.47766 - accuracy: 0.00000\n",
      "Iteration: 00004 - loss: 2.19723 - accuracy: 0.00000\n",
      "Iteration: 00005 - loss: 1.98520 - accuracy: 0.00000\n",
      "Iteration: 00006 - loss: 1.92280 - accuracy: 0.00000\n",
      "Iteration: 00007 - loss: 1.92815 - accuracy: 0.00000\n",
      "Iteration: 00008 - loss: 1.93227 - accuracy: 0.00000\n",
      "Iteration: 00009 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00010 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00011 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00012 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00013 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00014 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00015 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00016 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00017 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00018 - loss: nan - accuracy: 0.00000\n",
      "Iteration: 00019 - loss: nan - accuracy: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:122: RuntimeWarning: divide by zero encountered in true_divide\n",
      "D:\\miniconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in multiply\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "parameters = train(np.transpose(D_train), np.transpose(D_train), network_architecture, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "D_test_hat, _ = full_forward_propagation(np.transpose(D_test), parameters, network_architecture)\n",
    "\n",
    "# Accuracy achieved on the test set\n",
    "acc_test = Accuracy(D_test_hat, np.transpose(D_test.reshape((D_test.shape[0],1))))\n",
    "print(\"Test Set Accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
