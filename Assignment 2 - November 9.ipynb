{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Activations\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "## Backward Activations\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sigmoid_Z = sigmoid(Z)\n",
    "    return dA * sigmoid_Z * (1 - sigmoid_Z)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <=0] = 0\n",
    "    return dZ\n",
    "    \n",
    "## Initialize Layers\n",
    "def initialize_layers(network_architecture):\n",
    "    number_of_layers = len(network_architecture)\n",
    "    parameters = {}\n",
    "    \n",
    "    for index, layer in enumerate(network_architecture):\n",
    "        layer_index = index + 1\n",
    "        layer_input_size = layer[\"input_dimension\"]\n",
    "        layer_output_size = layer[\"output_dimension\"]\n",
    "        \n",
    "        ##### remove *0.1  to parameter initialization #####\n",
    "        parameters['W' + str(layer_index)] = np.random.randn(layer_output_size, layer_input_size)*0.01\n",
    "        parameters['b' + str(layer_index)] = np.random.randn(layer_output_size, 1)*0.01\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "def single_layer_forward_propagation(A_previous, W_current, b_current, activation=\"relu\"):\n",
    "    Z_current = np.dot(W_current, A_previous) + b_current\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_function = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_function = sigmoid\n",
    "    else:\n",
    "        raiseException('Unsupported activation')\n",
    "\n",
    "    return activation_function(Z_current), Z_current\n",
    "\n",
    "def full_forward_propagation(X, parameters, network_architecture):\n",
    "    memory = {}\n",
    "    A_current = X\n",
    "    \n",
    "    for index, layer in enumerate(network_architecture):\n",
    "        layer_index = index + 1\n",
    "        A_previous = A_current\n",
    "        \n",
    "        activation_function_current = layer[\"activation\"]\n",
    "        W_current = parameters[\"W\" + str(layer_index)]\n",
    "        b_current = parameters[\"b\" + str(layer_index)]\n",
    "        A_current, Z_current = single_layer_forward_propagation(A_previous, W_current, b_current, activation_function_current)\n",
    "\n",
    "        memory[\"A\" + str(index)] = A_previous\n",
    "        memory[\"Z\" + str(layer_index)] = Z_current\n",
    "        \n",
    "    return A_current, memory\n",
    "\n",
    "## Loss\n",
    "##### Check for correct implementation #####\n",
    "def MSE(Y_hat, Y):\n",
    "    loss = np.power(Y_hat - Y,2).mean()\n",
    "    return loss\n",
    "\n",
    "## Performance Metric\n",
    "def l2_distance(Y_hat, Y):\n",
    "    distance = np.sqrt(np.power(Y_hat - Y,2))\n",
    "    return distance.mean()\n",
    "\n",
    "## Backward Propagation\n",
    "##### MIGHT BE CHANGED TO FOLLOW DISCUSSION'S IMPLEMENTATION #####\n",
    "def single_layer_backward_propagation(dA_current, W_current, b_current, Z_current, A_previous, activation=\"relu\"):\n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_activation_function = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_function = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Unsupported activation')\n",
    "        \n",
    "    dZ_current = backward_activation_function(dA_current, Z_current)\n",
    "    \n",
    "    dW_current = np.dot(dZ_current, np.transpose(A_previous)) / m\n",
    "    db_current = np.sum(dZ_current, axis=1, keepdims=True) / m\n",
    "    dA_previous = np.dot(np.transpose(W_current), dZ_current)\n",
    "    \n",
    "    return dA_previous, dW_current, db_current\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, parameters, network_architecture):\n",
    "    gradients = {}\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    dA_previous = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
    "    \n",
    "    for layer_index_previous, layer in reversed(list(enumerate(network_architecture))):\n",
    "        layer_index_current = layer_index_previous + 1\n",
    "        activation_function_current = layer[\"activation\"]\n",
    "        \n",
    "        dA_current = dA_previous\n",
    "        \n",
    "        A_previous = memory[\"A\" + str(layer_index_previous)]\n",
    "        Z_current = memory[\"Z\" + str(layer_index_current)]\n",
    "        \n",
    "        W_current = parameters[\"W\" + str(layer_index_current)]\n",
    "        b_current = parameters[\"b\" + str(layer_index_current)]\n",
    "        \n",
    "        dA_previous, dW_current, db_current = single_layer_backward_propagation(dA_current, W_current, b_current, Z_current, A_previous, activation_function_current)\n",
    "        \n",
    "        gradients[\"dW\" + str(layer_index_current)] = dW_current\n",
    "        gradients[\"db\" + str(layer_index_current)] = db_current\n",
    "        \n",
    "    return gradients\n",
    "\n",
    "def update(parameters, gradients, network_architecture, learning_rate):\n",
    "    \n",
    "    for layer_index, layer in enumerate(network_architecture, 1):\n",
    "        parameters[\"W\" + str(layer_index)] -= learning_rate * gradients[\"dW\" + str(layer_index)]\n",
    "        parameters[\"b\" + str(layer_index)] -= learning_rate * gradients[\"db\" + str(layer_index)]\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def train(X, Y, network_architecture, epochs, learning_rate):\n",
    "    parameters = initialize_layers(network_architecture)\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cache = full_forward_propagation(X, parameters, network_architecture)\n",
    "        \n",
    "        ##### loss should be MSE ######\n",
    "        loss = MSE(Y_hat, Y)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        accuracy = l2_distance(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        gradients = full_backward_propagation(Y_hat, Y, cache, parameters, network_architecture)\n",
    "        \n",
    "        parameters = update(parameters, gradients, network_architecture, learning_rate)\n",
    "        \n",
    "        print(\"Iteration: {:02} - loss: {:.5f} - l2-distance: {:.5f}\".format(i+1, loss, accuracy))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network Architecture\n",
    "network_architecture = [\n",
    "    {\"input_dimension\":1, \"output_dimension\":64, \"activation\":\"relu\"},\n",
    "    {\"input_dimension\":64, \"output_dimension\":64, \"activation\":\"relu\"},\n",
    "    {\"input_dimension\":64, \"output_dimension\":1, \"activation\":\"sigmoid\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Mean:0\n",
      "Enter Standard Deviation:1\n"
     ]
    }
   ],
   "source": [
    "mean_text = input(\"Enter Mean:\")\n",
    "mean = float(mean_text)\n",
    "\n",
    "standard_deviation_text = input(\"Enter Standard Deviation:\")\n",
    "standard_deviation = float(standard_deviation_text)\n",
    "\n",
    "number_of_samples = 1000\n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "\n",
    "#### Fix Dataset Generation - should be between [-2*std,2*std] #####\n",
    "#### Recode \n",
    "\n",
    "X = np.random.uniform(mean - 2*standard_deviation, mean + 2*standard_deviation,(1000,1))\n",
    "Y = 1/(standard_deviation*np.sqrt(2*np.pi)) * np.exp(-(X - mean)/(-2*np.square(standard_deviation)))\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for i in range(number_of_samples):\n",
    "    if i % 10 == 0:\n",
    "        X_test.append(X[i])\n",
    "        Y_test.append(Y[i])\n",
    "    else:\n",
    "        X_train.append(X[i])\n",
    "        Y_train.append(Y[i])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 01 - loss: 0.06360 - l2-distance: 0.21726\n",
      "Iteration: 02 - loss: 0.06354 - l2-distance: 0.21707\n",
      "Iteration: 03 - loss: 0.06348 - l2-distance: 0.21688\n",
      "Iteration: 04 - loss: 0.06343 - l2-distance: 0.21670\n",
      "Iteration: 05 - loss: 0.06338 - l2-distance: 0.21653\n",
      "Iteration: 06 - loss: 0.06334 - l2-distance: 0.21636\n",
      "Iteration: 07 - loss: 0.06329 - l2-distance: 0.21620\n",
      "Iteration: 08 - loss: 0.06325 - l2-distance: 0.21604\n",
      "Iteration: 09 - loss: 0.06321 - l2-distance: 0.21588\n",
      "Iteration: 10 - loss: 0.06317 - l2-distance: 0.21573\n",
      "Iteration: 11 - loss: 0.06314 - l2-distance: 0.21559\n",
      "Iteration: 12 - loss: 0.06310 - l2-distance: 0.21545\n",
      "Iteration: 13 - loss: 0.06307 - l2-distance: 0.21531\n",
      "Iteration: 14 - loss: 0.06304 - l2-distance: 0.21518\n",
      "Iteration: 15 - loss: 0.06301 - l2-distance: 0.21505\n",
      "Iteration: 16 - loss: 0.06298 - l2-distance: 0.21493\n",
      "Iteration: 17 - loss: 0.06295 - l2-distance: 0.21481\n",
      "Iteration: 18 - loss: 0.06293 - l2-distance: 0.21470\n",
      "Iteration: 19 - loss: 0.06291 - l2-distance: 0.21458\n",
      "Iteration: 20 - loss: 0.06288 - l2-distance: 0.21448\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "parameters = train(np.transpose(X_train), np.transpose(Y_train), network_architecture, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Distance: 0.20998\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), parameters, network_architecture)\n",
    "\n",
    "# Accuracy achieved on the test set\n",
    "acc_test = l2_distance(Y_test_hat, np.transpose(Y_test))\n",
    "print(\"Test Set Distance: {:.5f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
